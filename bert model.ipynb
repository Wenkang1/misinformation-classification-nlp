{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train.json'\n",
    "json_file = open('train.json', encoding='utf-8') \n",
    "train_data = json.load(json_file)\n",
    "filename = 'dev.json'\n",
    "json_file = open('dev.json', encoding='utf-8') \n",
    "dev_data = json.load(json_file)\n",
    "\n",
    "filename = 'test-unlabelled.json'\n",
    "json_file = open(filename, encoding='utf-8') \n",
    "test_data = json.load(json_file)\n",
    "\n",
    "filename = 'dev-baseline-r.json'\n",
    "json_file = open(filename, encoding='utf-8') \n",
    "dev_baseline_data = json.load(json_file)\n",
    "\n",
    "filename = 'test-output.json'\n",
    "json_file = open(filename, encoding='utf-8') \n",
    "result_example = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "def extract_data(data):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    for event in data:\n",
    "        text_list.append(data[event]['text'])\n",
    "        if 'label' in data[event].keys():\n",
    "            label_list.append(data[event]['label'])\n",
    "\n",
    "    return text_list,label_list\n",
    "train_text_list, train_label_list = extract_data(train_data)\n",
    "dev_text_list, dev_label_list = extract_data(dev_data)\n",
    "test_text_list, test_label_list = extract_data(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('preprocessed_train_text.pickle', 'rb') as handle:\n",
    "    preprocessed_train_text = pickle.load(handle)\n",
    "with open('bow_train_text.pickle', 'rb') as handle:\n",
    "    bow_train_text = pickle.load(handle)\n",
    "print(len(preprocessed_train_text))\n",
    "with open('preprocessed_dev_text.pickle', 'rb') as handle:\n",
    "    preprocessed_dev_text = pickle.load(handle)\n",
    "with open('bow_dev_text.pickle', 'rb') as handle:\n",
    "    bow_dev_text = pickle.load(handle)\n",
    "print(len(preprocessed_dev_text))\n",
    "\n",
    "with open('preprocessed_test_text.pickle', 'rb') as handle:\n",
    "    preprocessed_test_text = pickle.load(handle)\n",
    "with open('bow_test_text.pickle', 'rb') as handle:\n",
    "    bow_test_text = pickle.load(handle)\n",
    "print(len(preprocessed_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import extra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "def get_extra_data(data_dir,num=50):\n",
    "    extra_dev_data = []\n",
    "    for event in sorted(os.listdir(data_dir))[:num]:\n",
    "        story_file = os.path.join(data_dir,event)\n",
    "        with open(story_file,encoding='utf-8') as st_file:\n",
    "            data = st_file.read()\n",
    "        extra_dev_data.append(data[:3000])\n",
    "        #st_file.close()\n",
    "    return extra_dev_data\n",
    "\n",
    "cnn_data_dir = 'C:\\\\Users\\\\pierce\\\\Desktop\\\\nlp\\\\project\\\\cnn\\\\stories'\n",
    "#dm_data_dir = 'C:\\\\Users\\\\pierce\\\\Desktop\\\\nlp\\\\project\\\\dailymail\\\\stories'\n",
    "extra_dev_data = get_extra_data(cnn_data_dir, num=100)\n",
    "#extra_dev_data_dm = get_extra_data(dm_data_dir)\n",
    "#extra_dev_data = extra_dev_data_cnn + extra_dev_data_dm\n",
    "dev_text_list = dev_text_list + extra_dev_data\n",
    "dev_text_list = dev_text_list[:200]\n",
    "print(len(dev_text_list))\n",
    "dev_label_list = dev_label_list+[0 for i in range(100)]\n",
    "dev_label_list = dev_label_list[:200]\n",
    "print(len(dev_label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##add extra data to dev\n",
    "extra_dev_bow, extra_dev_text = preprocess_bow(extra_dev_data)\n",
    "preprocessed_dev_text.extend(extra_dev_text)\n",
    "dev_label_list.extend([0 for i in range(100)])\n",
    "bow_dev_text.extend(extra_dev_bow)\n",
    "print(len(preprocessed_dev_text))\n",
    "print(len(dev_label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4168\n",
      "4168\n"
     ]
    }
   ],
   "source": [
    "#extra training data\n",
    "cnn_data_dir = 'C:\\\\Users\\\\pierce\\\\Desktop\\\\nlp\\\\project\\\\cnn\\\\stories'\n",
    "#dm_data_dir = 'C:\\\\Users\\\\pierce\\\\Desktop\\\\nlp\\\\project\\\\dailymail\\\\stories'\n",
    "extra_train_data_cnn = get_extra_data(cnn_data_dir, num=3100)\n",
    "#extra_train_data_dm = get_extra_data(dm_data_dir, num=1550)\n",
    "extra_train_data = train_text_list + extra_train_data_cnn[100:]\n",
    "train_text_list = extra_train_data[:(3000+len(train_data))]\n",
    "train_label_list = train_label_list+[0 for i in range(3000)]\n",
    "train_label_list = train_label_list[:(3000+len(train_data))]\n",
    "print(len(train_text_list))\n",
    "print(len(train_label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('train_text_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_text_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('train_label_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_label_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('dev_text_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(dev_text_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('dev_label_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(dev_label_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_text_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_text_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#add to training \n",
    "extra_train_bow, extra_train_text = preprocess_bow(extra_train_data)\n",
    "preprocessed_train_text.extend(extra_train_text)\n",
    "train_label_list.extend([0 for i in range(len(extra_train_data))])\n",
    "bow_train_text.extend(extra_train_bow)\n",
    "print(len(preprocessed_train_text))\n",
    "print(len(train_label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4168\n",
      "200\n",
      "1410\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('preprocessed_train_text.pickle', 'rb') as handle:\n",
    "    preprocessed_train_text = pickle.load(handle)\n",
    "print(len(preprocessed_train_text))\n",
    "with open('preprocessed_dev_text.pickle', 'rb') as handle:\n",
    "    preprocessed_dev_text = pickle.load(handle)\n",
    "print(len(preprocessed_dev_text))\n",
    "\n",
    "with open('preprocessed_test_text.pickle', 'rb') as handle:\n",
    "    preprocessed_test_text = pickle.load(handle)\n",
    "print(len(preprocessed_test_text))\n",
    "\n",
    "\n",
    "train_text_list = []\n",
    "for doc in preprocessed_train_text:\n",
    "    train_text_list.append(' '.join(doc))\n",
    "    \n",
    "dev_text_list = []\n",
    "for doc in preprocessed_dev_text:\n",
    "    dev_text_list.append(' '.join(doc))\n",
    "    \n",
    "test_text_list = []\n",
    "for doc in preprocessed_test_text:\n",
    "    test_text_list.append(' '.join(doc))\n",
    "    \n",
    "dev_label_list = dev_label_list+[0 for i in range(100)]\n",
    "dev_label_list = dev_label_list[:200]\n",
    "train_label_list = train_label_list+[0 for i in range(3000)]\n",
    "train_label_list = train_label_list[:(3000+len(train_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_preprocessing(dev_text_list):\n",
    "    processed_list = []\n",
    "    for text in dev_text_list:\n",
    "        text = re.sub('@[^\\s]+','',text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r\"http\\S+\", \"\",text)\n",
    "        text = re.sub(r\"cnn\", \"\",text)\n",
    "        text =  re.sub(r\"[']\", '', text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"there's\", \"there is\", text)\n",
    "        text = re.sub(r\"We're\", \"We are\", text)\n",
    "        text = re.sub(r\"That's\", \"That is\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"they're\", \"they are\", text)\n",
    "        text = re.sub(r\"Can't\", \"Cannot\", text)\n",
    "        text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "        text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
    "        text = re.sub(r\"aren't\", \"are not\", text)\n",
    "        text = re.sub(r\"isn't\", \"is not\", text)\n",
    "        text = re.sub(r\"What's\", \"What is\", text)\n",
    "        text = re.sub(r\"haven't\", \"have not\", text)\n",
    "        text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "        text = re.sub(r\"There's\", \"There is\", text)\n",
    "        text = re.sub(r\"He's\", \"He is\", text)\n",
    "        text = re.sub(r\"It's\", \"It is\", text)\n",
    "        text = re.sub(r\"You're\", \"You are\", text)\n",
    "        text = re.sub(r\"I'M\", \"I am\", text)\n",
    "        text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "        text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "        text = re.sub(r\"i'm\", \"I am\", text)\n",
    "        text = re.sub(r\"I'm\", \"I am\", text)\n",
    "        text = re.sub(r\"Isn't\", \"is not\", text)\n",
    "        text = re.sub(r\"Here's\", \"Here is\", text)\n",
    "        text = re.sub(r\"you've\", \"you have\", text)\n",
    "        text = re.sub(r\"we're\", \"we are\", text)\n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "        text = re.sub(r\"we've\", \"we have\", text)\n",
    "        text = re.sub(r\"who's\", \"who is\", text)\n",
    "        text = re.sub(r\"y'all\", \"you all\", text)\n",
    "        text = re.sub(r\"would've\", \"would have\", text)\n",
    "        text = re.sub(r\"it'll\", \"it will\", text)\n",
    "        text = re.sub(r\"we'll\", \"we will\", text)\n",
    "        text = re.sub(r\"We've\", \"We have\", text)\n",
    "        text = re.sub(r\"he'll\", \"he will\", text)\n",
    "        text = re.sub(r\"Y'all\", \"You all\", text)\n",
    "        text = re.sub(r\"Weren't\", \"Were not\", text)\n",
    "        text = re.sub(r\"Didn't\", \"Did not\", text)\n",
    "        text = re.sub(r\"they'll\", \"they will\", text)\n",
    "        text = re.sub(r\"they'd\", \"they would\", text)\n",
    "        text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
    "        text = re.sub(r\"they've\", \"they have\", text)\n",
    "        text = re.sub(r\"i'd\", \"I would\", text)\n",
    "        text = re.sub(r\"should've\", \"should have\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"we'd\", \"we would\", text)\n",
    "        text = re.sub(r\"i'll\", \"I will\", text)\n",
    "        text = re.sub(r\"weren't\", \"were not\", text)\n",
    "        text = re.sub(r\"They're\", \"They are\", text)\n",
    "        text = re.sub(r\"let's\", \"let us\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"you're\", \"you are\", text)\n",
    "        text = re.sub(r\"i've\", \"I have\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"i'll\", \"I will\", text)\n",
    "        text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "        text = re.sub(r\"i'd\", \"I would\", text)\n",
    "        text = re.sub(r\"didn't\", \"did not\", text)\n",
    "        text = re.sub(r\"ain't\", \"am not\", text)\n",
    "        text = re.sub(r\"you'll\", \"you will\", text)\n",
    "        text = re.sub(r\"I've\", \"I have\", text)\n",
    "        text = re.sub(r\"Don't\", \"do not\", text)\n",
    "        text = re.sub(r\"I'll\", \"I will\", text)\n",
    "        text = re.sub(r\"I'd\", \"I would\", text)\n",
    "        text = re.sub(r\"Let's\", \"Let us\", text)\n",
    "        text = re.sub(r\"you'd\", \"You would\", text)\n",
    "        text = re.sub(r\"It's\", \"It is\", text)\n",
    "        text = re.sub(r\"Ain't\", \"am not\", text)\n",
    "        text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "        text = re.sub(r\"Could've\", \"Could have\", text)\n",
    "        text = re.sub(r\"youve\", \"you have\", text)  \n",
    "        text = re.sub(r\"don’t\", \"do not\", text)\n",
    "        text = re.sub(r\"I’m\", \"I am\", text)\n",
    "        text = re.sub(r\"you’ve\", \"you have\", text)\n",
    "        text = re.sub(r\"it’s\", \"it is\", text)\n",
    "        text = re.sub(r\"doesn’t\", \"does not\", text)\n",
    "        text = re.sub(r\"It’s\", \"It is\", text)\n",
    "        text = re.sub(r\"Here’s\", \"Here is\", text)\n",
    "        text = re.sub(r\"I’ve\", \"I have\", text)\n",
    "        text = re.sub(r\"can’t\", \"cannot\", text)\n",
    "        text = re.sub(r\"wouldn’t\", \"would not\", text)\n",
    "        text = re.sub(r\"That’s\", \"That is\", text)\n",
    "        text = re.sub(r\"You’re\", \"You are\", text)\n",
    "        text = re.sub(r\"Don’t\", \"Do not\", text)\n",
    "        text = re.sub(r\"Can’t\", \"Cannot\", text)\n",
    "        text = re.sub(r\"you’ll\", \"you will\", text)\n",
    "        text = re.sub(r\"I’d\", \"I would\", text)\n",
    "        text = re.sub(r\"don’t\", \"do not\", text)   \n",
    "        text = re.sub(r\"there’s\", \"there is\", text)\n",
    "\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        text = re.sub(r\"“|”\", \"\", text)\n",
    "        text = re.sub(r\"‘|’\", \"\", text)\n",
    "\n",
    "        text = re.sub(r\"cnn\", \"\",text)\n",
    "        text = re.sub(r\"CNN\", \"\",text)\n",
    "        text = text.replace(u'\\xa0', u' ')\n",
    "        text = text.replace(u'\\u200b', '')\n",
    "        text = text.replace(u'‐', '')\n",
    "        text = text.replace(u'‑', '')\n",
    "        text = text.replace(u'—', '')\n",
    "        text = text.replace(u'–', '')\n",
    "        text = text.replace(u'…', '')\n",
    "        \n",
    "    \n",
    "        processed_list.append(text)\n",
    "        \n",
    "    return processed_list\n",
    "\n",
    "\n",
    "processed_dev_list = pre_preprocessing(dev_text_list)\n",
    "processed_train_list = pre_preprocessing(train_text_list)\n",
    "processed_test_list = pre_preprocessing(test_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_split(text_list):\n",
    "    splitted_list = []\n",
    "    for text1 in text_list:\n",
    "        l_total = []\n",
    "        l_parcial = []\n",
    "        if len(text1.split())//150 >0:\n",
    "            n = len(text1.split())//150\n",
    "        else: \n",
    "            n = 1\n",
    "        for w in range(n):\n",
    "            if w == 0:\n",
    "                l_parcial = text1.split()[:200]\n",
    "                l_total.append(\" \".join(l_parcial))\n",
    "            else:\n",
    "                l_parcial = text1.split()[w*150:w*150 + 200]\n",
    "                l_total.append(\" \".join(l_parcial))\n",
    "        splitted_list.append(l_total)\n",
    "    return splitted_list\n",
    "\n",
    "splitted_train_list = get_split(processed_train_list)\n",
    "splitted_dev_list = get_split(processed_dev_list)\n",
    "splitted_test_list = get_split(processed_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "####################\n",
    "def get_bertid(sentences, maxlen=300):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        if len(tokens) < maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:int(maxlen*0.3)] + tokens[-int(maxlen*0.7):] #head 80 tokens tail 220 tokens\n",
    "            tokens = tokens[:maxlen-1] + ['[SEP]']\n",
    "\n",
    "        tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        input_ids.append(tokens_ids_tensor)\n",
    "        attention_masks.append(attn_mask)\n",
    "\n",
    "    input_ids = torch.stack(input_ids) #list of tensor to tensor\n",
    "    attention_masks = torch.stack(attention_masks)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks = get_bertid(processed_train_list, maxlen=64)\n",
    "dev_input_ids, dev_attention_masks = get_bertid(processed_dev_list, maxlen=64)\n",
    "test_input_ids, test_attention_masks = get_bertid(processed_test_list, maxlen=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_labels = torch.tensor(train_label_list)\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "\n",
    "dev_labels = torch.tensor(dev_label_list)\n",
    "dev_dataset = TensorDataset(dev_input_ids, dev_attention_masks, dev_labels)\n",
    "\n",
    "#test data no label\n",
    "t_labels = [0 for i in range(len(test_input_ids))]\n",
    "t_labels = torch.tensor(t_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(dev_dataset,batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "print('We will use the GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "  \n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        #nn.init.xavier_normal_(self.classifier.weight)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "configuration = BertConfig()\n",
    "#configuration.output_hidden_states=True\n",
    "print(configuration)\n",
    "#net = SentimentClassifier()\n",
    "net = BertForSequenceClassification(configuration)  \n",
    "#    output_attentions = False, # Whether the model returns attentions weights.\n",
    "#    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "#)\n",
    "net.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = AdamW(net.parameters(), lr=5e-6, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_clf_evaluate(net, validation_dataloader, gpu):\n",
    "  net.eval()\n",
    "  loss_list = []\n",
    "  pred_label_list = []\n",
    "  with torch.no_grad():\n",
    "    for seq, attn_masks, labels in validation_dataloader:\n",
    "      seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "      loss,logits = net(seq, attention_mask=attn_masks, labels=labels)\n",
    "      prob = torch.sigmoid(logits.unsqueeze(-1)).cpu()\n",
    "      pred_label = np.argmax(prob, axis=1).flatten().tolist()\n",
    "      pred_label_list.extend(pred_label)\n",
    "      labels = labels.cpu()\n",
    "      loss_list.append(loss.cpu())\n",
    "  print('validation result prediction:',pred_label_list)\n",
    "  print(print(classification_report(dev_label_list,pred_label_list)))\n",
    "  print('avg loss', np.average(loss_list))\n",
    "\n",
    "def clf_evaluate(net,criterion, validation_dataloader, gpu):\n",
    "  net.eval()\n",
    "  pred_label_list = []\n",
    "  mean_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for seq, attn_masks, labels in validation_dataloader:\n",
    "      seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "      logits = net(seq, attn_masks)\n",
    "      mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "      prob = torch.sigmoid(logits).cpu()\n",
    "      pred_label = (prob>0.5).long().flatten().tolist()\n",
    "      pred_label_list.extend(pred_label)\n",
    "      labels = labels.cpu().tolist()\n",
    "  print('validation result prediction:',pred_label_list)\n",
    "  print(print(classification_report(dev_label_list,pred_label_list)))\n",
    "  print('mean loss', mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 1.46 GiB already allocated; 0 bytes free; 1.53 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-06864164194d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m#Optimization step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mopti\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;31m#scheduler.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lr\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 1.46 GiB already allocated; 0 bytes free; 1.53 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "epochs = 3\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "#with torch.no_grad():\n",
    "# For each epoch...\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    bmodel.train()\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        #clear previously calculated gradients\n",
    "        bmodel.zero_grad()  \n",
    "        #Obtaining the logits from the model\n",
    "        loss, logits = bmodel(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        #Computing loss\n",
    "        total_train_loss += loss.item()\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(bmodel.parameters(), 1.0)\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    \n",
    "    seq_clf_evaluate(bmodel, validation_dataloader, device)\n",
    "    #print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(epoch_i, dev_acc, dev_loss))\n",
    "    #if dev_acc > best_acc:\n",
    "    #    print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "    #    best_acc = dev_acc\n",
    "     #   torch.save(bmodel.state_dict(), 'sstcls_{}.dat'.format(epoch_i))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#for embedding \n",
    "#####################\n",
    "class TextNet(nn.Module):\n",
    "  \n",
    "    def __init__(self, output_dim):\n",
    "        super(TextNet, self).__init__()\n",
    "        #self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        #self.fc = nn.Linear(768, output_dim)\n",
    "        #self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        text_embeddings = output[0][:,0,:]\n",
    "        #features = self.fc(text_embeddings)\n",
    "        #features = self.tanh(features)\n",
    "\n",
    "        return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TextNet(80) # bert embedding model\n",
    "net.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert sequential embedding\n",
    "def bert_embedding_features(train_dataloader):\n",
    "        gpu = device\n",
    "        net.eval()\n",
    "        loss_list = []\n",
    "        pred_label_list = []\n",
    "        embedding_list = []\n",
    "        with torch.no_grad():\n",
    "          final_features = np.empty((0, 768))\n",
    "          for seq, attn_masks, labels in train_dataloader:\n",
    "              seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "              features = net(seq, attention_mask=attn_masks)\n",
    "              features = features.cpu().numpy()\n",
    "              final_features = np.append(final_features, features, axis=0)\n",
    "          #bert_embedding = hidden_state[0].cpu()\n",
    "          #embedding_list.append(bert_embedding)\n",
    "        #embedding_result = torch.stack(embedding_list)\n",
    "        return final_features\n",
    "train_bert_feature = bert_embedding_features(train_dataloader)\n",
    "dev_bert_feature = bert_embedding_features(validation_dataloader)\n",
    "test_bert_feature = bert_embedding_features(test_dataloader)\n",
    "print(train_bert_feature.shape)\n",
    "print(dev_bert_feature.shape)\n",
    "print(test_bert_feature.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "def train_ocsvm(train_set_d2v,dev_set_d2v,nu=0.3):\n",
    "    svm_clf = OneClassSVM(kernel=\"rbf\", degree=1, gamma='auto', coef0=0.0,\n",
    "                           tol=0.0000001, nu=nu, shrinking=True, cache_size=200, \n",
    "                           max_iter=-1)\n",
    "    svm_clf.fit(train_set_d2v)\n",
    "    dev_predict = svm_clf.predict(dev_set_d2v)\n",
    "    dev_predict[dev_predict == -1] = 0\n",
    "    print(accuracy_score(dev_label_list,dev_predict))\n",
    "    print(f1_score(dev_label_list, dev_predict, labels=None, pos_label=1, average='binary'))\n",
    "    print(classification_report(dev_label_list,dev_predict))\n",
    "    return svm_clf \n",
    "#svm_clf = train_ocsvm(train_set_d2v,dev_set_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = train_ocsvm(train_bert_feature,dev_bert_feature,nu=0.12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
