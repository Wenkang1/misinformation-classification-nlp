{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train.json'\n",
    "json_file = open('train.json') \n",
    "train_data = json.load(json_file)\n",
    "filename = 'dev.json'\n",
    "json_file = open('dev.json') \n",
    "dev_data = json.load(json_file)\n",
    "\n",
    "filename = 'test-unlabelled.json'\n",
    "json_file = open(filename) \n",
    "test_data = json.load(json_file)\n",
    "\n",
    "filename = 'dev-baseline-r.json'\n",
    "json_file = open(filename) \n",
    "dev_baseline_data = json.load(json_file)\n",
    "\n",
    "filename = 'test-output.json'\n",
    "json_file = open(filename) \n",
    "result_example = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "def extract_data(data):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    for event in data:\n",
    "        text_list.append(data[event]['text'])\n",
    "        if 'label' in data[event].keys():\n",
    "            label_list.append(data[event]['label'])\n",
    "\n",
    "    return text_list,label_list\n",
    "\n",
    "train_text_list, train_label_list = extract_data(train_data)\n",
    "dev_text_list, dev_label_list = extract_data(dev_data)\n",
    "test_text_list, test_label_list = extract_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tree import Tree\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "def get_lemmatized(string):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tag = pos_tag([string])[0][1]\n",
    "    wordnet_tag = get_wordnet_pos(tag)\n",
    "    result = lemmatizer.lemmatize(string,wordnet_tag)\n",
    "    return result\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \n",
    "    \"\"\"Map treebank tag to wordnet tag\"\"\"\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "def is_number(n):\n",
    "    try:\n",
    "        float(n)   \n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocess_bow(train_text_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #ps = PorterStemmer() \n",
    "    punctuation_sets = set(\"\\\"\\\"#‘’$“”%&()+-./ :`\\'\\';<=>@[\\],，^_`{|}~+\")\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    preprocessed_train_text = []\n",
    "    bow_train_text = []\n",
    "    \n",
    "    for text in train_text_list:\n",
    "        text = re.sub('@[^\\s]+','',text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r\"http\\S+\", \"\",text)\n",
    "        text = word_tokenize(text)\n",
    "        text_no_sword = [n for n in text if n.lower() not in stopwords_set|punctuation_sets ]\n",
    "        #print(text_no_sword)\n",
    "        chunk = ne_chunk(pos_tag(text_no_sword))\n",
    "        text_list = []\n",
    "        bow_event= defaultdict(int)\n",
    "        for n in chunk:\n",
    "           # print('org n',n)\n",
    "            if type(n) == Tree:\n",
    "                #n = n.label()\n",
    "                continue\n",
    "            #elif is_number(n[0]):\n",
    "                #n = 'number'\n",
    "            #    continue\n",
    "            #elif n[0][-1].lower() == 'c' or n[0][-1].lower() == 'f':\n",
    "            #    n = 'temperature'\n",
    "            elif len(n[0]) <= 2 and n[0] not in '?!':\n",
    "                continue \n",
    "            elif len(n[0]) > 12:\n",
    "                continue\n",
    "            else:\n",
    "                #n = ps.stem(n[0].lower())\n",
    "                n = get_lemmatized(n[0].lower().strip())\n",
    "                \n",
    "            n =  re.sub(r\"[']\", '', n)\n",
    "            text_list.append(n)\n",
    "            bow_event[n] += 1\n",
    "            \n",
    "        bow_train_text.append(bow_event)\n",
    "        preprocessed_train_text.append(text_list)\n",
    "\n",
    "    return bow_train_text,preprocessed_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing and save result to pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "bow_train_text,preprocessed_train_text = preprocess_bow(train_text_list)\n",
    "bow_dev_text,preprocessed_dev_text = preprocess_bow(dev_text_list)\n",
    "bow_test_text,preprocessed_test_text = preprocess_bow(test_text_list)\n",
    "\n",
    "\n",
    "with open('preprocessed_train_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(preprocessed_train_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('bow_train_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(bow_train_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('preprocessed_dev_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(preprocessed_dev_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('bow_dev_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(bow_dev_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('preprocessed_test_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(preprocessed_test_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('bow_test_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(bow_test_text, handle, protocol=pickle.HIGHEST_PROTOCOL)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import preprocessed data from pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('preprocessed_train_text.pickle', 'rb') as handle:\n",
    "    preprocessed_train_text = pickle.load(handle)\n",
    "with open('bow_train_text.pickle', 'rb') as handle:\n",
    "    bow_train_text = pickle.load(handle)\n",
    "print(len(preprocessed_train_text))\n",
    "with open('preprocessed_dev_text.pickle', 'rb') as handle:\n",
    "    preprocessed_dev_text = pickle.load(handle)\n",
    "with open('bow_dev_text.pickle', 'rb') as handle:\n",
    "    bow_dev_text = pickle.load(handle)\n",
    "print(len(preprocessed_dev_text))\n",
    "\n",
    "with open('preprocessed_test_text.pickle', 'rb') as handle:\n",
    "    preprocessed_test_text = pickle.load(handle)\n",
    "with open('bow_test_text.pickle', 'rb') as handle:\n",
    "    bow_test_text = pickle.load(handle)\n",
    "print(len(preprocessed_test_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_word_mis = ''\n",
    "all_word_nonmis = ''\n",
    "non_ind = [i for i,x in enumerate(dev_label_list) if x == 0]\n",
    "non_dex_text = [preprocessed_dev_text[i] for i in non_ind]\n",
    "for doc in preprocessed_train_text:\n",
    "    all_word_mis += ' '.join(doc)\n",
    "for doc in non_dex_text:\n",
    "    all_word_nonmis += ' '.join(doc)\n",
    "    \n",
    "\n",
    "non_ind = [i for i,x in enumerate(dev_label_list) if x == 0]\n",
    "non_dex_text_raw = [dev_text_list[i] for i in non_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cloud = WordCloud(max_font_size=60).generate(all_word_mis)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cloud = WordCloud(max_font_size=60).generate(all_word_nonmis)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "train_set = vectorizer.fit_transform(bow_train_text)\n",
    "dev_set = vectorizer.transform(bow_dev_text)\n",
    "test_set = vectorizer.transform(bow_test_text)\n",
    "print('train set dimension',train_set.shape)\n",
    "print('dev set dimension',dev_set.shape)\n",
    "print('test set dimension',test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "def train_ocsvm(train_set_d2v,dev_set_d2v,nu=0.3):\n",
    "    svm_clf = OneClassSVM(kernel=\"rbf\", degree=1, gamma='auto', coef0=0.0,\n",
    "                           tol=0.0000001, nu=nu, shrinking=True, cache_size=200, \n",
    "                           max_iter=-1)\n",
    "    svm_clf.fit(train_set_d2v)\n",
    "    dev_predict = svm_clf.predict(dev_set_d2v)\n",
    "    dev_predict[dev_predict == -1] = 0\n",
    "    print(accuracy_score(dev_label_list,dev_predict))\n",
    "    print(f1_score(dev_label_list, dev_predict, labels=None, pos_label=1, average='binary'))\n",
    "    print(classification_report(dev_label_list,dev_predict))\n",
    "    return svm_clf \n",
    "svm_clf = train_ocsvm(train_set,dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameter tune on nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list_svm = []\n",
    "prev_acc= 0 \n",
    "for alpha in range(25,30):\n",
    "    svm_clf = OneClassSVM(kernel=\"rbf\", degree=1, gamma='auto', coef0=0.0,\n",
    "                       tol=0.0000001, nu=alpha/100, shrinking=True, cache_size=200, \n",
    "                       max_iter=-1)\n",
    "    svm_clf.fit(train_set)\n",
    "    dev_predict = svm_clf.predict(dev_set)\n",
    "    dev_predict[dev_predict == -1] = 0\n",
    "    acc = f1_score(dev_label_list, dev_predict, labels=None, pos_label= 1, average='binary')\n",
    "    acc_list_svm.append(acc)\n",
    "    if acc > prev_acc:\n",
    "        prev_acc = acc\n",
    "        param = alpha/100\n",
    "print(acc_list_svm)\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(350)\n",
    "train_set_svd = svd.fit_transform(train_set)\n",
    "dev_set_svd = svd.transform(dev_set)\n",
    "test_set_svd = svd.transform(test_set)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "svm_clf = train_ocsvm(train_set_svd,dev_set_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result_json(model,test_set_svd):\n",
    "    test_predict = model.predict(test_set_svd)\n",
    "    test_predict[test_predict == -1] = 0\n",
    "    print(sum(test_predict))\n",
    "    result = dict()\n",
    "    i = 0\n",
    "    for n in test_data:\n",
    "        result[n] = {'label': int(test_predict[i])}\n",
    "        i+=1\n",
    "    with open('test-output.json', 'w') as file:\n",
    "        json.dump(result, file)\n",
    "\n",
    "write_result_json(svm_clf,test_set_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred = svm_clf.predict(dev_set)\n",
    "dev_pred[dev_pred == -1] = 0\n",
    "error_ind = [i for i in range(100) if dev_pred[i] != dev_label_list[i]]\n",
    "\n",
    "label1_dev = [i for i in error_ind if dev_label_list[i] ==1] #label真实为1 的被错误分到了 label 0\n",
    "label0_dev = [i for i in error_ind if dev_label_list[i] ==0] #label真实为0 的被错误分到了 label 1\n",
    "\n",
    "error_text_l1 = [dev_text_list[i] for i in range(100) if i in label1_dev] \n",
    "error_text_l0 = [dev_text_list[i] for i in range(100) if i in label0_dev]\n",
    "error_bow_vecl1 = [dev_set[i] for i in range(100) if i in label1_dev]\n",
    "error_bow_vecl0 = [dev_set[i] for i in range(100) if i in label0_dev]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
